{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uploading to Hugging Face datasets\n",
    "\n",
    "This is the notebook used to upload the HAM10000 dataset to hugging face (HF).\n",
    "\n",
    "Original dataset from kaggle was extracted to data folder.\n",
    "\n",
    "More information on formats and how to's to upload to HF datasets can be found [here](https://huggingface.co/docs/datasets/image_dataset#loading-script)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oem/miniconda3/envs/skin_cancer/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import jsonlines\n",
    "from datasets import load_dataset\n",
    "from torchvision import transforms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Read in files and join with Metadata\n",
    "\n",
    "Download the HAM10000_metadata.csv file to categorize the datasets by their diagnosis type "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/oem/Documents/coding/personal/skin_cancer'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# moving the directory to the root folder, optional step since this notebook was in notebook folder\n",
    "os.chdir('..')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      file_name      image_id    lesion_id  \\\n",
      "0  data/ham10000_images_part_2/ISIC_0031774.jpg  ISIC_0031774  HAM_0002275   \n",
      "1  data/ham10000_images_part_2/ISIC_0030527.jpg  ISIC_0030527  HAM_0006713   \n",
      "2  data/ham10000_images_part_2/ISIC_0033561.jpg  ISIC_0033561  HAM_0004708   \n",
      "3  data/ham10000_images_part_2/ISIC_0034041.jpg  ISIC_0034041  HAM_0005496   \n",
      "4  data/ham10000_images_part_2/ISIC_0031369.jpg  ISIC_0031369  HAM_0000531   \n",
      "\n",
      "                 dx    dx_type   age     sex     localization  \n",
      "0  melanocytic_Nevi  follow_up  45.0  female  lower extremity  \n",
      "1  melanocytic_Nevi  follow_up  50.0  female            trunk  \n",
      "2  melanocytic_Nevi      histo  45.0    male            trunk  \n",
      "3  melanocytic_Nevi      histo  15.0  female  lower extremity  \n",
      "4          melanoma      histo  85.0    male             face  \n",
      "(10015, 8)\n"
     ]
    }
   ],
   "source": [
    "path = 'data'\n",
    "fullpath = os.path.join(os.getcwd(), path)\n",
    "\n",
    "# walking through the directory to get the path names\n",
    "datapath = []\n",
    "for root, _, files in os.walk(fullpath):\n",
    "    for file in files:\n",
    "        datapath.append(os.path.relpath(os.path.join(root, file)))\n",
    "\n",
    "orig_df = pd.DataFrame(pd.Series(datapath))\n",
    "orig_df = orig_df.rename(columns={0: 'file_name'})\n",
    "orig_df['image_id'] = orig_df[\"file_name\"].apply(lambda x: os.path.splitext(os.path.basename(x))[0])\n",
    "\n",
    "\n",
    "# add mapping the image names to metadata diagnosis\n",
    "meta_df = pd.read_csv(os.path.join(os.getcwd(), 'notebooks/HAM10000_metadata.csv'))\n",
    "lesion_type_dict = {\n",
    "    'nv': 'melanocytic_Nevi',\n",
    "    'mel': 'melanoma',\n",
    "    'bkl': 'benign_keratosis-like_lesions',\n",
    "    'bcc': 'basal_cell_carcinoma',\n",
    "    'akiec': 'actinic_keratoses',\n",
    "    'vasc': 'vascular_lesions',\n",
    "    'df': 'dermatofibroma'\n",
    "}\n",
    "meta_df['dx'] = meta_df.dx.map(lesion_type_dict)\n",
    "\n",
    "df = orig_df.merge(meta_df, how='inner', left_on='image_id', right_on='image_id')\n",
    "print(df.head())\n",
    "df.to_csv(\"dataset.csv\", index=False)\n",
    "print(df.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Split files by directory\n",
    "\n",
    "Using SKlearn's train test split to split dataset into train test and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df\n",
    "y = df['dx']\n",
    "\n",
    "# initial split to train and validation set\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, stratify=y)\n",
    "\n",
    "# further split validation set into validation and test set\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_valid, y_valid, train_size=(2/3), stratify=y_valid)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Move files to correct folders\n",
    "\n",
    "Creating the necessary metadata.jsonl file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {'train': X_train, 'valid': X_valid, 'test': X_test}\n",
    "\n",
    "# creating the new destination path in the dataset\n",
    "for k, v in datasets.items():\n",
    "    v['base_path'] = v['file_name'].apply(lambda x: os.path.split(x)[1])\n",
    "    v['move_path'] = 'data' + os.path.sep + k + os.path.sep + v['dx'] + os.path.sep + v['base_path']\n",
    "    v = v.drop(columns=['base_path'])\n",
    "\n",
    "# creating the necessary folders for train,test,split\n",
    "for k in datasets.keys():\n",
    "    parentfolderpath = os.path.join(os.getcwd(), 'data', k)\n",
    "    if os.path.isdir(parentfolderpath)==False:\n",
    "        os.mkdir(parentfolderpath)\n",
    "\n",
    "# creating the necessary subfolder for each cancer type\n",
    "for col in df['dx'].unique():\n",
    "    for k in datasets.keys():\n",
    "        folderpath = os.path.join(os.getcwd(), 'data', k, col)\n",
    "        if os.path.isdir(folderpath)==False:\n",
    "                os.mkdir(folderpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# moving the files to their correct destination\n",
    "for k, v in datasets.items():\n",
    "    for i, row in v.iterrows():\n",
    "        filename = os.path.join(os.getcwd(), row['file_name'])\n",
    "        movepath = os.path.join(os.getcwd(), row['move_path'])\n",
    "        shutil.move(filename, movepath)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Create Jsonl files\n",
    "\n",
    "Please note that this is not a standard json file as each line is a json dictionary. Use jsonlines to convert and dump the necessary files into metadata.jsonl "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the jsonlines files\n",
    "for k, v in datasets.items():\n",
    "    # editing the dataset to get only the folder and filename in \"file_name\" column\n",
    "    v['filepath'] = v['move_path'].copy()\n",
    "    v['foldername'] = v['filepath'].apply(lambda x: x.split(os.path.sep)[-2])\n",
    "    v['filename'] = v['move_path'].apply(lambda x: os.path.basename(x))\n",
    "    v['file_name'] = v['foldername'] + os.path.sep + v['filename']\n",
    "    v = v.drop(columns=['filepath', 'move_path', 'foldername', 'base_path', 'filename'])\n",
    "\n",
    "    # creating the jsonlines file\n",
    "    res = v.to_json(orient='records')\n",
    "    jsonls = json.loads(res)\n",
    "    jsonobj = json.dumps(jsonls)\n",
    "    with jsonlines.open(os.path.join(os.getcwd(), 'data', k, 'metadata.jsonl'), 'w') as writer:\n",
    "        writer.write_all(jsonls)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Upload to Hugging Face Hub\n",
    "\n",
    "In order to upload to HF hub, you will first need to set up an account [here](https://huggingface.co/welcome)\n",
    "\n",
    "You will also need to install git lfs by installing the latest version of git, or following the steps [here](https://stackoverflow.com/questions/48734119/git-lfs-is-not-a-git-command-unclear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resolving data files: 100%|██████████| 9603/9603 [00:01<00:00, 5980.73it/s] \n",
      "Resolving data files: 100%|██████████| 1297/1297 [00:00<00:00, 35350.46it/s]\n",
      "Resolving data files: 100%|██████████| 2492/2492 [00:01<00:00, 2233.80it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'transform'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 21\u001b[0m\n\u001b[1;32m      2\u001b[0m transform \u001b[39m=\u001b[39m {\n\u001b[1;32m      3\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m: transforms\u001b[39m.\u001b[39mCompose([\n\u001b[1;32m      4\u001b[0m                 transforms\u001b[39m.\u001b[39mResize([\u001b[39m224\u001b[39m,\u001b[39m224\u001b[39m]),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m             ]),\n\u001b[1;32m     18\u001b[0m }\n\u001b[1;32m     20\u001b[0m \u001b[39m# upload dataset to hugging face\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m dataset \u001b[39m=\u001b[39m load_dataset(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(os\u001b[39m.\u001b[39;49mgetcwd(), \u001b[39m\"\u001b[39;49m\u001b[39mdata\u001b[39;49m\u001b[39m\"\u001b[39;49m), transform\u001b[39m=\u001b[39;49mtransform)\n\u001b[1;32m     22\u001b[0m dataset\u001b[39m.\u001b[39mpush_to_hub(\u001b[39m\"\u001b[39m\u001b[39mmarmal88/skin_cancer\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/skin_cancer/lib/python3.9/site-packages/datasets/load.py:1734\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, num_proc, **config_kwargs)\u001b[0m\n\u001b[1;32m   1731\u001b[0m ignore_verifications \u001b[39m=\u001b[39m ignore_verifications \u001b[39mor\u001b[39;00m save_infos\n\u001b[1;32m   1733\u001b[0m \u001b[39m# Create a dataset builder\u001b[39;00m\n\u001b[0;32m-> 1734\u001b[0m builder_instance \u001b[39m=\u001b[39m load_dataset_builder(\n\u001b[1;32m   1735\u001b[0m     path\u001b[39m=\u001b[39;49mpath,\n\u001b[1;32m   1736\u001b[0m     name\u001b[39m=\u001b[39;49mname,\n\u001b[1;32m   1737\u001b[0m     data_dir\u001b[39m=\u001b[39;49mdata_dir,\n\u001b[1;32m   1738\u001b[0m     data_files\u001b[39m=\u001b[39;49mdata_files,\n\u001b[1;32m   1739\u001b[0m     cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   1740\u001b[0m     features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[1;32m   1741\u001b[0m     download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[1;32m   1742\u001b[0m     download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[1;32m   1743\u001b[0m     revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m   1744\u001b[0m     use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m   1745\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mconfig_kwargs,\n\u001b[1;32m   1746\u001b[0m )\n\u001b[1;32m   1748\u001b[0m \u001b[39m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[1;32m   1749\u001b[0m \u001b[39mif\u001b[39;00m streaming:\n",
      "File \u001b[0;32m~/miniconda3/envs/skin_cancer/lib/python3.9/site-packages/datasets/load.py:1518\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, use_auth_token, **config_kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(error_msg)\n\u001b[1;32m   1517\u001b[0m \u001b[39m# Instantiate the dataset builder\u001b[39;00m\n\u001b[0;32m-> 1518\u001b[0m builder_instance: DatasetBuilder \u001b[39m=\u001b[39m builder_cls(\n\u001b[1;32m   1519\u001b[0m     cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   1520\u001b[0m     config_name\u001b[39m=\u001b[39;49mconfig_name,\n\u001b[1;32m   1521\u001b[0m     data_dir\u001b[39m=\u001b[39;49mdata_dir,\n\u001b[1;32m   1522\u001b[0m     data_files\u001b[39m=\u001b[39;49mdata_files,\n\u001b[1;32m   1523\u001b[0m     \u001b[39mhash\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mhash\u001b[39;49m,\n\u001b[1;32m   1524\u001b[0m     features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[1;32m   1525\u001b[0m     use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m   1526\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mbuilder_kwargs,\n\u001b[1;32m   1527\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mconfig_kwargs,\n\u001b[1;32m   1528\u001b[0m )\n\u001b[1;32m   1530\u001b[0m \u001b[39mreturn\u001b[39;00m builder_instance\n",
      "File \u001b[0;32m~/miniconda3/envs/skin_cancer/lib/python3.9/site-packages/datasets/builder.py:1357\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder.__init__\u001b[0;34m(self, writer_batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1356\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, writer_batch_size\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m-> 1357\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1358\u001b[0m     \u001b[39m# Batch size used by the ArrowWriter\u001b[39;00m\n\u001b[1;32m   1359\u001b[0m     \u001b[39m# It defines the number of samples that are kept in memory before writing them\u001b[39;00m\n\u001b[1;32m   1360\u001b[0m     \u001b[39m# and also the length of the arrow chunks\u001b[39;00m\n\u001b[1;32m   1361\u001b[0m     \u001b[39m# None means that the ArrowWriter will use its default value\u001b[39;00m\n\u001b[1;32m   1362\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_writer_batch_size \u001b[39m=\u001b[39m writer_batch_size \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mDEFAULT_WRITER_BATCH_SIZE\n",
      "File \u001b[0;32m~/miniconda3/envs/skin_cancer/lib/python3.9/site-packages/datasets/builder.py:322\u001b[0m, in \u001b[0;36mDatasetBuilder.__init__\u001b[0;34m(self, cache_dir, config_name, hash, base_path, info, features, use_auth_token, repo_id, data_files, data_dir, name, **config_kwargs)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[39mif\u001b[39;00m data_dir \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    321\u001b[0m     config_kwargs[\u001b[39m\"\u001b[39m\u001b[39mdata_dir\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m data_dir\n\u001b[0;32m--> 322\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig_id \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_builder_config(\n\u001b[1;32m    323\u001b[0m     config_name\u001b[39m=\u001b[39;49mconfig_name,\n\u001b[1;32m    324\u001b[0m     custom_features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[1;32m    325\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mconfig_kwargs,\n\u001b[1;32m    326\u001b[0m )\n\u001b[1;32m    328\u001b[0m \u001b[39m# prepare info: DatasetInfo are a standardized dataclass across all datasets\u001b[39;00m\n\u001b[1;32m    329\u001b[0m \u001b[39m# Prefill datasetinfo\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \u001b[39mif\u001b[39;00m info \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/skin_cancer/lib/python3.9/site-packages/datasets/builder.py:475\u001b[0m, in \u001b[0;36mDatasetBuilder._create_builder_config\u001b[0;34m(self, config_name, custom_features, **config_kwargs)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mversion\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m config_kwargs \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mVERSION\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mVERSION:\n\u001b[1;32m    474\u001b[0m         config_kwargs[\u001b[39m\"\u001b[39m\u001b[39mversion\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mVERSION\n\u001b[0;32m--> 475\u001b[0m     builder_config \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mBUILDER_CONFIG_CLASS(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mconfig_kwargs)\n\u001b[1;32m    477\u001b[0m \u001b[39m# otherwise use the config_kwargs to overwrite the attributes\u001b[39;00m\n\u001b[1;32m    478\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    479\u001b[0m     builder_config \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(builder_config)\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'transform'"
     ]
    }
   ],
   "source": [
    "# load the transformer to change PIL to tensor\n",
    "transform = {\n",
    "    'train': transforms.Compose([\n",
    "                transforms.Resize([224,224]),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=(0.5,0.5,0.5), std=(0.5,0.5,0.5)),\n",
    "            ]),\n",
    "    'validation': transforms.Compose([\n",
    "                transforms.Resize([224,224]),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=(0.5,0.5,0.5), std=(0.5,0.5,0.5)),\n",
    "            ]),\n",
    "    'test': transforms.Compose([\n",
    "                transforms.Resize([224,224]),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=(0.5,0.5,0.5), std=(0.5,0.5,0.5)),\n",
    "            ]),\n",
    "}\n",
    "\n",
    "# upload dataset to hugging face\n",
    "dataset = load_dataset(os.path.join(os.getcwd(), \"data\"), revision=\"resize_244x244\", transform=transform)\n",
    "dataset.push_to_hub(\"marmal88/skin_cancer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skin_cancer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a7455868454a19ea7dd6c625dbb606deb8fae83cfa0f0f84e5c0ef92922f1079"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
