{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uploading to Hugging Face datasets\n",
    "\n",
    "This is the notebook used to upload the HAM10000 dataset to hugging face (HF).\n",
    "\n",
    "Original dataset from kaggle was extracted to data folder.\n",
    "\n",
    "More information on formats and how to's to upload to HF datasets can be found [here](https://huggingface.co/docs/datasets/image_dataset#loading-script)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oem/miniconda3/envs/skin_cancer/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import jsonlines\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Read in files and join with Metadata\n",
    "\n",
    "Download the HAM10000_metadata.csv file to categorize the datasets by their diagnosis type "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# moving the directory to the root folder\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataframe shape: (20030, 2)\n",
      "Meta dataframe shape: (10015, 7)\n"
     ]
    }
   ],
   "source": [
    "path = 'data'\n",
    "fullpath = os.path.join(os.getcwd(), path)\n",
    "\n",
    "# walking through the directory to get the path names\n",
    "datapath = []\n",
    "for root, _, files in os.walk(fullpath):\n",
    "    for file in files:\n",
    "        datapath.append(os.path.relpath(os.path.join(root, file)))\n",
    "\n",
    "orig_df = pd.DataFrame(pd.Series(datapath))\n",
    "orig_df = orig_df.rename(columns={0: 'file_name'})\n",
    "orig_df['image_id'] = orig_df[\"file_name\"].apply(lambda x: os.path.splitext(os.path.basename(x))[0])\n",
    "\n",
    "\n",
    "# add mapping the image names to metadata diagnosis\n",
    "meta_df = pd.read_csv(os.path.join(os.getcwd(), 'notebooks/HAM10000_metadata.csv'))\n",
    "lesion_type_dict = {\n",
    "    'nv': 'melanocytic_Nevi',\n",
    "    'mel': 'melanoma',\n",
    "    'bkl': 'benign_keratosis-like_lesions',\n",
    "    'bcc': 'basal_cell_carcinoma',\n",
    "    'akiec': 'actinic_keratoses',\n",
    "    'vasc': 'vascular_lesions',\n",
    "    'df': 'dermatofibroma'\n",
    "}\n",
    "meta_df['dx'] = meta_df.dx.map(lesion_type_dict)\n",
    "\n",
    "df = orig_df.merge(meta_df, how='inner', left_on='image_id', right_on='image_id')\n",
    "print(df.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Split files by directory\n",
    "\n",
    "Using SKlearn's train test split to split dataset into train test and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df\n",
    "y = df['dx']\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, stratify=y)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_valid, y_valid, train_size=(2/3), stratify=y_valid)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Move files to correct folders\n",
    "\n",
    "Creating the necessary metadata.jsonl file. \n",
    "\n",
    "Please note that this is not a standard json file as each line is a json dictionary. Use jsonlines to convert and dump the necessary files into metadata.jsonl "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {'train': X_train, 'valid': X_valid, 'test': X_test}\n",
    "\n",
    "# creating the new destination path in the dataset\n",
    "for k, v in datasets.items():\n",
    "    v['base_path'] = v['file_name'].apply(lambda x: os.path.split(x)[1])\n",
    "    v['move_path'] = 'data' + os.path.sep + k + os.path.sep + v['dx'] + os.path.sep + v['base_path']\n",
    "    v = v.drop(columns=['base_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the necessary folders for train,test,split\n",
    "for k in datasets.keys():\n",
    "    parentfolderpath = os.path.join(os.getcwd(), 'data', k)\n",
    "    if os.path.isdir(parentfolderpath)==False:\n",
    "        os.mkdir(parentfolderpath)\n",
    "\n",
    "# creating the necessary subfolder for each cancer type\n",
    "for col in df['dx'].unique():\n",
    "    for k in datasets.keys():\n",
    "        folderpath = os.path.join(os.getcwd(), 'data', k, col)\n",
    "        if os.path.isdir(folderpath)==False:\n",
    "                os.mkdir(folderpath)\n",
    "\n",
    "# moving the files to their correct destination\n",
    "for k, v in datasets.items():\n",
    "    for i, row in v.iterrows():\n",
    "        filename = os.path.join(os.getcwd(), row['file_name'])\n",
    "        movepath = os.path.join(os.getcwd(), row['move_path'])\n",
    "        shutil.move(filename, movepath)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Jsonl files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the jsonlines files\n",
    "for k, v in datasets.items():\n",
    "    # editing the dataset to get only the folder and filename in \"file_name\" column\n",
    "    v['filepath'] = v['move_path'].copy()\n",
    "    v['foldername'] = v['filepath'].apply(lambda x: x.split(os.path.sep)[-2])\n",
    "    v['filename'] = v['move_path'].apply(lambda x: os.path.basename(x))\n",
    "    v['file_name'] = v['foldername'] + os.path.sep + v['filename']\n",
    "    v = v.drop(columns=['filepath', 'move_path', 'foldername', 'base_path', 'filename'])\n",
    "\n",
    "    # creating the jsonlines file\n",
    "    res = v.to_json(orient='records')\n",
    "    jsonls = json.loads(res)\n",
    "    jsonobj = json.dumps(jsonls)\n",
    "    with jsonlines.open(os.path.join(os.getcwd(), 'data', k, 'metadata.jsonl'), 'w') as writer:\n",
    "        writer.write_all(jsonls)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload to Hugging Face Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resolving data files: 100%|██████████| 9578/9578 [00:00<00:00, 16333.04it/s]\n",
      "Resolving data files: 100%|██████████| 1286/1286 [00:00<00:00, 22729.62it/s]\n",
      "Resolving data files: 100%|██████████| 2493/2493 [00:00<00:00, 11224.38it/s]\n",
      "Using custom data configuration data-87648cf40e2c2d6c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset imagefolder/data to /home/oem/.cache/huggingface/datasets/imagefolder/data-87648cf40e2c2d6c/0.0.0/37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 9581/9581 [00:00<00:00, 70904.13it/s]\n",
      "Downloading data files: 0it [00:00, ?it/s]\n",
      "Extracting data files: 0it [00:00, ?it/s]\n",
      "Downloading data files: 100%|██████████| 1289/1289 [00:00<00:00, 74699.59it/s]\n",
      "Downloading data files: 0it [00:00, ?it/s]\n",
      "Extracting data files: 0it [00:00, ?it/s]\n",
      "Downloading data files: 100%|██████████| 2496/2496 [00:00<00:00, 75562.68it/s]\n",
      "Downloading data files: 0it [00:00, ?it/s]\n",
      "Extracting data files: 0it [00:00, ?it/s]\n",
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset imagefolder downloaded and prepared to /home/oem/.cache/huggingface/datasets/imagefolder/data-87648cf40e2c2d6c/0.0.0/37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 16.98it/s]\n",
      "Pushing split train to the Hub.\n",
      "100%|██████████| 2/2 [00:02<00:00,  1.28s/ba]\n",
      "100%|██████████| 2/2 [00:02<00:00,  1.00s/ba]0%|██        | 1/5 [03:08<12:35, 188.87s/it]\n",
      "100%|██████████| 2/2 [00:02<00:00,  1.25s/ba]0%|████      | 2/5 [05:24<07:52, 157.39s/it]\n",
      "100%|██████████| 2/2 [00:03<00:00,  1.62s/ba]0%|██████    | 3/5 [07:35<04:50, 145.25s/it]\n",
      "100%|██████████| 2/2 [00:03<00:00,  1.84s/ba]0%|████████  | 4/5 [09:48<02:20, 140.45s/it]\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 5/5 [12:01<00:00, 144.38s/it]\n",
      "Pushing split test to the Hub.\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.61ba/s]\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 1/1 [02:08<00:00, 128.22s/it]\n",
      "Pushing split validation to the Hub.\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.77ba/s]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.16ba/s]0%|█████     | 1/2 [01:29<01:29, 89.22s/it]\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 2/2 [04:12<00:00, 126.23s/it]\n"
     ]
    }
   ],
   "source": [
    "# upload dataset to hugging face\n",
    "\n",
    "dataset = load_dataset(os.path.join(os.getcwd(), \"data\"))\n",
    "dataset.push_to_hub(\"marmal88/skin_cancer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skin_cancer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a7455868454a19ea7dd6c625dbb606deb8fae83cfa0f0f84e5c0ef92922f1079"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
